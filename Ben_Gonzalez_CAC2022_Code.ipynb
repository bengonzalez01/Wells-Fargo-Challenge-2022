{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pilO-WCATaqb"
   },
   "outputs": [],
   "source": [
    "# General data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing, sampling, cross validation, and data manipulation packages\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import tensorflow_hub as hub # For importing sentence encoder model\n",
    "\n",
    "# Other libraries\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Checking Memory Usage\n",
    "\n",
    "In a big-data environment these functions are useful in checking RAM usage and knowing when to manually use Garbage Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hnWu52p_Taqc"
   },
   "outputs": [],
   "source": [
    "# Script to view Memory for Garbage Collection \n",
    "def obj_size_fmt(num):\n",
    "    \"\"\"Function that allows for human readable memory size of an object\n",
    "    Parameters: number\n",
    "    Returns: Size taken up in memort in a human readable way\"\"\"\n",
    "    if num<10**3:\n",
    "        return \"{:.2f}{}\".format(num,\"B\")\n",
    "    elif ((num>=10**3)&(num<10**6)):\n",
    "        return \"{:.2f}{}\".format(num/(1.024*10**3),\"KB\")\n",
    "    elif ((num>=10**6)&(num<10**9)):\n",
    "        return \"{:.2f}{}\".format(num/(1.024*10**6),\"MB\")\n",
    "    else:\n",
    "        return \"{:.2f}{}\".format(num/(1.024*10**9),\"GB\")\n",
    "\n",
    "def memory_usage():\n",
    "    \"\"\"Function that shows a dataframe of top objects memory usage\n",
    "    Parameters: none\n",
    "    Returns: Dataframe showing objects taking up the most memory\"\"\"\n",
    "    memory_usage_by_variable=pd.DataFrame({k:sys.getsizeof(v)\\\n",
    "    for (k,v) in globals().items()},index=['Size'])\n",
    "    memory_usage_by_variable=memory_usage_by_variable.T\n",
    "    \n",
    "    memory_usage_by_variable=memory_usage_by_variable.sort_values(by='Size',ascending=False).head(10)\n",
    "   \n",
    "    memory_usage_by_variable['Size']=memory_usage_by_variable['Size'].apply(lambda x: obj_size_fmt(x))\n",
    "    \n",
    "    return memory_usage_by_variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical and Numeric Preprocessing\n",
    "\n",
    "1. Importing the data from the excel files\n",
    "2. Dropping useless columns and dealing with null values\n",
    "3. Encoding categorical variables\n",
    "4. Ensuring that training and unlabeled dataset have identical columns\n",
    "5. Changing datatypes as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LGTyUsIiTaqd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Columns: 291 entries, onehotencoder__x0_BK to Category\n",
      "dtypes: object(291)\n",
      "memory usage: 88.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 291 entries, onehotencoder__x0_BK to Category\n",
      "dtypes: object(291)\n",
      "memory usage: 22.2+ MB\n",
      "None\n",
      "10837\n"
     ]
    }
   ],
   "source": [
    "# Importing training set and unlabeled data\n",
    "df = pd.read_excel('CAC-2022-Training-Data-Set-New.xlsx')\n",
    "df_test = pd.read_excel('CAC-2022-Test-Data-Set-New.xlsx')\n",
    "\n",
    "# Dropping useless columns from both datasets\n",
    "df.drop(['cdf_seq_no','payment_reporting_category'], axis=1, inplace=True)\n",
    "df_test.drop(['cdf_seq_no','payment_reporting_category'], axis=1, inplace=True)\n",
    "\n",
    "# Dealing with null values by creating placeholder variable for both training and unlabeled\n",
    "df['merchant_cat_code'] = df['merchant_cat_code'].fillna(-1)\n",
    "df['merchant_cat_code'] = df['merchant_cat_code'].astype(str)\n",
    "df['db_cr_cd'] = df['db_cr_cd'].fillna('O')\n",
    "\n",
    "df_test['merchant_cat_code'] = df['merchant_cat_code'].fillna(-1)\n",
    "df_test['merchant_cat_code'] = df['merchant_cat_code'].astype(str)\n",
    "df_test['db_cr_cd'] = df['db_cr_cd'].fillna('O')\n",
    "\n",
    "\n",
    "# Encoding all categorical variables using a column transformer\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(sparse=False), ['sor','merchant_cat_code', 'db_cr_cd', 'payment_category']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "# Applying the transformer to both the training and the unlabled set\n",
    "transformed = transformer.fit_transform(df)\n",
    "transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())\n",
    "\n",
    "transformed_test = transformer.fit_transform(df_test)\n",
    "transformed_df_test = pd.DataFrame(transformed_test, columns=transformer.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "# Dropping the placeholder columns from both datasets\n",
    "transformed_df.drop(['onehotencoder__x2_O','onehotencoder__x1_-1.0'],axis=1,inplace=True)\n",
    "transformed_df_test.drop(['onehotencoder__x2_O','onehotencoder__x1_-1.0'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Ensuring that the training and unlabeled have the same columns by dropping training-set-only columns\n",
    "transformed_list_x1 = []\n",
    "for i in transformed_df.columns:\n",
    "    if i[0:17] == 'onehotencoder__x1':\n",
    "        transformed_list_x1.append(i)\n",
    "        \n",
    "transformed_list_test_x1 = []\n",
    "for i in transformed_df_test.columns:\n",
    "    if i[0:17] == 'onehotencoder__x1':\n",
    "        transformed_list_test_x1.append(i)\n",
    "        \n",
    "\n",
    "del_list = np.setdiff1d(transformed_list_x1,transformed_list_test_x1)\n",
    "transformed_df.drop(del_list,axis=1,inplace=True)\n",
    "\n",
    "# Double checking to make sure that the number of columns is identical\n",
    "print(transformed_df.info())\n",
    "print(transformed_df_test.info())\n",
    "\n",
    "\n",
    "\n",
    "transformed_df_test = transformed_df_test[transformed_df.columns]\n",
    "df = transformed_df\n",
    "df_test = transformed_df_test\n",
    "\n",
    "\n",
    "# Changing datatype of 'is_international' column to int\n",
    "df['is_international'] = df['is_international'].astype(int)\n",
    "df_test['is_international'] = df_test['is_international'].astype(int)\n",
    "\n",
    "print(df_test.isna().sum().sum())\n",
    "# Dropping all null values and reseting index on training and unlabeled\n",
    "df.dropna(inplace= True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "1. Selecting text columns to manipulate\n",
    "2. Making all letters lowercase\n",
    "3. Removing special characters and numbers\n",
    "4. Removing all stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "smdqtv3ITaqe"
   },
   "outputs": [],
   "source": [
    "# Importing stop-words and naming text columns\n",
    "stop_words = get_stop_words('en')\n",
    "list_of_text = ['trans_desc', 'default_brand', 'default_location', 'qrated_brand', 'coalesced_brand']\n",
    "\n",
    "def remove_stopWords(s):\n",
    "    \"\"\"Removes the stopwords aka'common words' from a string\n",
    "    Parameters: string (input string of any length)\n",
    "    Returns: string (output string with all english stopwords removed)\"\"\"\n",
    "    s = ' '.join(word for word in s.split() if word not in stop_words)\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_text(dataframe):\n",
    "    \"\"\"Preprocesses text columns within a datframe\n",
    "    Parameters: Dataframe with text columns\n",
    "    Returns: Dataframe with text columns that are all lowercase, void of numbers, special characters and stopwords\"\"\"\n",
    "    for col in list_of_text:\n",
    "        dataframe[col] = dataframe[col].astype(str)\n",
    "        dataframe.loc[:,col] = dataframe[col].apply(lambda x : str.lower(x))\n",
    "        dataframe.loc[:,col] = dataframe[col].apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "        dataframe.loc[:,col] = dataframe[col].apply(lambda x: remove_stopWords(x))\n",
    "        dataframe[col] = dataframe[col].str.replace('\\d+', '')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vmhLJAhyTaqf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-c767197a8239>:24: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataframe[col] = dataframe[col].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "df = preprocess_text(df)\n",
    "df_test = preprocess_text(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Collection\n",
    "\n",
    "Removing large unnecessary objects that are taking up memory space to increase efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PIHUU52XTaqf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>df</th>\n",
       "      <td>342.92MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df_test</th>\n",
       "      <td>93.08MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>del_list</th>\n",
       "      <td>10.12KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformed_list_x1</th>\n",
       "      <td>3.22KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_iii</th>\n",
       "      <td>2.75KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_i3</th>\n",
       "      <td>2.75KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformed_list_test_x1</th>\n",
       "      <td>2.47KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_words</th>\n",
       "      <td>1.62KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_i4</th>\n",
       "      <td>1.18KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ii</th>\n",
       "      <td>1.18KB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Size\n",
       "df                        342.92MB\n",
       "df_test                    93.08MB\n",
       "del_list                   10.12KB\n",
       "transformed_list_x1         3.22KB\n",
       "_iii                        2.75KB\n",
       "_i3                         2.75KB\n",
       "transformed_list_test_x1    2.47KB\n",
       "stop_words                  1.62KB\n",
       "_i4                         1.18KB\n",
       "_ii                         1.18KB"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collection\n",
    "del transformed_df\n",
    "del transformed\n",
    "del transformed_df_test\n",
    "del transformed_test\n",
    "gc.collect()\n",
    "memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Universal Sentence Encoder\n",
    "\n",
    "1. Creating new dataframes of only text columns\n",
    "2. Loading Universal-Sentence-Encoder-Large from TensorFlow Hub\n",
    "3. Encoding all of the text for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0mR_kU-QTaqf"
   },
   "outputs": [],
   "source": [
    "#creating dataframes of text\n",
    "df_text = df[['trans_desc', 'default_brand', 'default_location', 'qrated_brand', 'coalesced_brand']]\n",
    "df_test_text = df_test[['trans_desc', 'default_brand', 'default_location', 'qrated_brand', 'coalesced_brand']]\n",
    "\n",
    "# Downloading tensorflow universal sentence encoder model\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7SpYF0MuTaqg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-37fad9d7739a>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_df[i] = encoded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with trans_desc\n",
      "Done with default_brand\n",
      "Done with default_location\n",
      "Done with qrated_brand\n",
      "Done with coalesced_brand\n",
      "Done with trans_desc\n",
      "Done with default_brand\n",
      "Done with default_location\n",
      "Done with qrated_brand\n",
      "Done with coalesced_brand\n"
     ]
    }
   ],
   "source": [
    "def encode(text_df):\n",
    "    for i in list_of_text:\n",
    "        embeddings = embed(text_df[i])\n",
    "        #create list from np arrays\n",
    "        encoded= np.array(embeddings).tolist()\n",
    "        #add lists as dataframe column\n",
    "        text_df[i] = encoded\n",
    "        #check dataframe\n",
    "        print(f'Done with {i}')\n",
    "\n",
    "    return text_df\n",
    "\n",
    "df_text = encode(df_text)\n",
    "df_test_text = encode(df_test_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Text Columns with Numeric and Categorical Columns\n",
    "\n",
    "1. Manipulate Encoded 'Text' from being a column of 512 dimension vectors so that each dimension of each vector is an individual column\n",
    "2. Combine these newly created columns with the previously created categorical and numeric columns\n",
    "\n",
    " **This will increase the dimension of the dataframe to over 2,800**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K2UGx6R-Taqg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in list_of_text:\n",
    "    # Looping through columns and making each dimension of the vector into its own column\n",
    "    column_names = []\n",
    "    for j in range(512):\n",
    "        col = f\"{i}{j}\"\n",
    "        column_names.append(col)\n",
    "    \n",
    "    split_df = pd.DataFrame(df_text[i].tolist(), columns = column_names)\n",
    "    split_df_test = pd.DataFrame(df_test_text[i].tolist(), columns = column_names)\n",
    "    \n",
    "    df_text = pd.concat([df_text, split_df], axis = 1)\n",
    "    df_test_text = pd.concat([df_test_text, split_df_test], axis = 1)\n",
    "\n",
    "# Garbage Collection for memory management\n",
    "del split_df\n",
    "del split_df_test\n",
    "gc.collect()\n",
    "\n",
    "# Concatenating x and y\n",
    "df_text.drop(labels=list_of_text, axis=1, inplace=True)\n",
    "df_test_text.drop(labels=list_of_text, axis=1, inplace=True)\n",
    "\n",
    "df.drop(labels=list_of_text, axis=1, inplace=True)\n",
    "df_test.drop(labels=list_of_text, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Creating final dataframe for training and labeling\n",
    "final_df = pd.concat([df_text, df], axis=1)\n",
    "del df_text\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "final_df_test = pd.concat([df_test_text, df_test], axis=1)\n",
    "del df_test_text\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ovykh4eFawzU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36667 entries, 0 to 36666\n",
      "Columns: 2846 entries, trans_desc0 to Category\n",
      "dtypes: float64(2560), int64(1), object(285)\n",
      "memory usage: 796.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 2846 entries, trans_desc0 to Category\n",
      "dtypes: float64(2560), int64(1), object(285)\n",
      "memory usage: 217.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Testing to make sure the dataframes are consistent\n",
    "print(final_df.info())\n",
    "print(final_df_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Model Training and Testing\n",
    "\n",
    "1. Create x and y variables by dropping the target variable 'Category'\n",
    "2. Split the labeled data into training and testing with a 80/20 split\n",
    "3. Loop through 6 base classification algorithms and analyze the accuracy\n",
    "4. Choose the top two best performing algorithms (Logistic Regression and Multi-Layer Perceptron) to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AO6roZiEirSD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating x and y\n",
    "y = final_df['Category'].values\n",
    "x = final_df.drop(labels=['Category'], axis=1)\n",
    "\n",
    "# Creating unlabeled x\n",
    "x_unlabeled_df = final_df_test.drop(labels=['Category'], axis=1)\n",
    "\n",
    "# Splitting data with 80/20 split and scaling data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_unlabeled = scaler.transform(x_unlabeled_df)\n",
    "\n",
    "# Manual Garbage Collection\n",
    "del final_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AbiINVXeTaqh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Nearest Neighbors: 0.7114807744750478\n",
      "Accuracy of Decision Tree: 0.5775838560130897\n",
      "Accuracy of Random Forest: 0.6994818652849741\n",
      "Accuracy of Feed Forward Neural Network: 0.7688846468502863\n",
      "Accuracy of Gaussian Naive Bayes: 0.10389964548677393\n",
      "Accuracy of Logistic Regression: 0.7428415598581947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamingonzalez/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Feed Forward Neural Network\",\n",
    "    \"Gaussian Naive Bayes\",\n",
    "    \"Logistic Regression\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(),\n",
    "]\n",
    "\n",
    "# Looping through classifiers and reporting accuracy\n",
    "for name, clf in zip(names, classifiers):\n",
    "        clf.fit(x_train, y_train)\n",
    "        score = clf.score(x_test, y_test)\n",
    "        print(f'Accuracy of {name}: {score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpfZalN0Y1T2"
   },
   "source": [
    "# Optimization of Logistic Regression Hyperparameters\n",
    "1. Create map of hyperparameters (specifically solvers and c-values)\n",
    "2. Utilize GridSearch with Cross Validation to report the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Y4u9lGbqYzpz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:  2.2min remaining:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:  2.2min remaining:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   8 | elapsed:  2.2min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   8 | elapsed:  9.0min remaining:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   8 | elapsed:  9.0min remaining:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 13.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 13.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.777675 using {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.728393 (0.004329) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.761257 (0.001848) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.761093 (0.001466) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.777675 (0.000061) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scale_x = scaler.fit_transform(x)\n",
    "del x\n",
    "gc.collect()\n",
    "# Logistic Regression\n",
    "model = LogisticRegression()\n",
    "solvers = ['lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, cv=2, scoring='accuracy',n_jobs=-1, verbose=10)\n",
    "grid_result = grid_search.fit(scale_x, y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtk-q1ZFZNXE"
   },
   "source": [
    "# Optimization of Neural Network Hyperparameters\n",
    "\n",
    "1. Create a parameter space with different amount of nodes in hidden layers\n",
    "2. Utilize GridSearch with Cross Validation to report the best performing model\n",
    "\n",
    "**Because the best performing model is a 400 neuron single layer MLP, use this to predict unlabeled values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nS66eJckZUzb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed: 10.3min remaining: 30.8min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed: 10.4min remaining: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed: 11.6min remaining:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed: 13.3min remaining:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 14.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7949277338423779\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (400,), 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier()\n",
    "\n",
    "# Define parameter space\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(500,), (400,), (300,) ,(300,200)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001]\n",
    "}\n",
    "\n",
    "# Utilize GridSearch\n",
    "clf_grid = GridSearchCV(clf, parameter_space, cv=3,n_jobs=-1, verbose=10)\n",
    "clf_grid.fit(x_train, y_train)\n",
    "print(clf_grid.score(x_test, y_test))\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYWrMJg9bIUl"
   },
   "source": [
    "# Using Optimized Model to Predict Unlabeled Data\n",
    "\n",
    "1. Train the model selected (from previous cell) on training data and report testing accuracy\n",
    "2. Use this model to predict unlabeled data\n",
    "3. Append predictions to dataframe\n",
    "4. Output newly labeled dataframe to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7912462503408781\n"
     ]
    }
   ],
   "source": [
    "# Training Model\n",
    "clf = MLPClassifier(activation='relu', alpha=.0001, hidden_layer_sizes=(400,), solver='adam')\n",
    "clf.fit(x_train, y_train)\n",
    "print(f'Testing Accuracy: {clf.score(x_test, y_test)}')\n",
    "\n",
    "# Predicting unlabeled data\n",
    "predictions = clf.predict(x_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Communication Services' 'Communication Services' 'Retail Trade' ...\n",
      " 'Travel' 'Travel' 'Retail Trade']\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = pd.read_excel('CAC-2022-Test-Data-Set-New.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sor</th>\n",
       "      <th>cdf_seq_no</th>\n",
       "      <th>trans_desc</th>\n",
       "      <th>merchant_cat_code</th>\n",
       "      <th>amt</th>\n",
       "      <th>db_cr_cd</th>\n",
       "      <th>payment_reporting_category</th>\n",
       "      <th>payment_category</th>\n",
       "      <th>is_international</th>\n",
       "      <th>default_brand</th>\n",
       "      <th>default_location</th>\n",
       "      <th>qrated_brand</th>\n",
       "      <th>coalesced_brand</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HH</td>\n",
       "      <td>T20131230990668080055738</td>\n",
       "      <td>CHECK CRD PURCHASE 11/11 PACKAGE EXPRESS      ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.35</td>\n",
       "      <td>D</td>\n",
       "      <td>Card</td>\n",
       "      <td>Check Card</td>\n",
       "      <td>False</td>\n",
       "      <td>PACKAGE EXPRESS</td>\n",
       "      <td>LACEY WA</td>\n",
       "      <td>Package Express</td>\n",
       "      <td>Package Express</td>\n",
       "      <td>Communication Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HH</td>\n",
       "      <td>T201302289918775816</td>\n",
       "      <td>RECUR DEBIT CRD PMT11/11 YP *FRMLY AT&amp;T AD    ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.00</td>\n",
       "      <td>D</td>\n",
       "      <td>Card</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>False</td>\n",
       "      <td>YP *FRMLY AT&amp;T AD</td>\n",
       "      <td>111-111-1111 CA</td>\n",
       "      <td>At And T</td>\n",
       "      <td>At And T</td>\n",
       "      <td>Communication Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HH</td>\n",
       "      <td>T20130726991361190218055</td>\n",
       "      <td>CHECK CRD PURCHASE 11/11 NORMAN G JENSEN IN   ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.00</td>\n",
       "      <td>D</td>\n",
       "      <td>Card</td>\n",
       "      <td>Check Card</td>\n",
       "      <td>False</td>\n",
       "      <td>NORMAN G JENSEN IN</td>\n",
       "      <td>111-1111111 MN</td>\n",
       "      <td>Norman G Jensen</td>\n",
       "      <td>Norman G Jensen</td>\n",
       "      <td>Retail Trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HH</td>\n",
       "      <td>T201208319924922772</td>\n",
       "      <td>CHECK CRD PUR RTRN 11/11 TWILIO               ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.00</td>\n",
       "      <td>C</td>\n",
       "      <td>Card</td>\n",
       "      <td>Check Card</td>\n",
       "      <td>False</td>\n",
       "      <td>TWILIO</td>\n",
       "      <td>SAN FARANSICO CA</td>\n",
       "      <td>Twilio</td>\n",
       "      <td>Twilio</td>\n",
       "      <td>Communication Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HH</td>\n",
       "      <td>T20131230990638080027066</td>\n",
       "      <td>CHECK CRD PURCHASE 11/11 AT&amp;T D11K 1111       ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>325.78</td>\n",
       "      <td>D</td>\n",
       "      <td>Card</td>\n",
       "      <td>Check Card</td>\n",
       "      <td>False</td>\n",
       "      <td>AT&amp;T D11K 1111</td>\n",
       "      <td>FORT WORTH TX</td>\n",
       "      <td>At And T</td>\n",
       "      <td>At And T</td>\n",
       "      <td>Communication Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sor                cdf_seq_no  \\\n",
       "0  HH  T20131230990668080055738   \n",
       "1  HH       T201302289918775816   \n",
       "2  HH  T20130726991361190218055   \n",
       "3  HH       T201208319924922772   \n",
       "4  HH  T20131230990638080027066   \n",
       "\n",
       "                                          trans_desc  merchant_cat_code  \\\n",
       "0  CHECK CRD PURCHASE 11/11 PACKAGE EXPRESS      ...                NaN   \n",
       "1  RECUR DEBIT CRD PMT11/11 YP *FRMLY AT&T AD    ...                NaN   \n",
       "2  CHECK CRD PURCHASE 11/11 NORMAN G JENSEN IN   ...                NaN   \n",
       "3  CHECK CRD PUR RTRN 11/11 TWILIO               ...                NaN   \n",
       "4  CHECK CRD PURCHASE 11/11 AT&T D11K 1111       ...                NaN   \n",
       "\n",
       "      amt db_cr_cd payment_reporting_category payment_category  \\\n",
       "0   10.35        D                       Card       Check Card   \n",
       "1   36.00        D                       Card       Debit Card   \n",
       "2   27.00        D                       Card       Check Card   \n",
       "3   20.00        C                       Card       Check Card   \n",
       "4  325.78        D                       Card       Check Card   \n",
       "\n",
       "   is_international       default_brand  default_location     qrated_brand  \\\n",
       "0             False     PACKAGE EXPRESS          LACEY WA  Package Express   \n",
       "1             False   YP *FRMLY AT&T AD   111-111-1111 CA         At And T   \n",
       "2             False  NORMAN G JENSEN IN    111-1111111 MN  Norman G Jensen   \n",
       "3             False              TWILIO  SAN FARANSICO CA           Twilio   \n",
       "4             False      AT&T D11K 1111     FORT WORTH TX         At And T   \n",
       "\n",
       "   coalesced_brand                Category  \n",
       "0  Package Express  Communication Services  \n",
       "1         At And T  Communication Services  \n",
       "2  Norman G Jensen            Retail Trade  \n",
       "3           Twilio  Communication Services  \n",
       "4         At And T  Communication Services  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append Predictions to Dataframe\n",
    "predict_data.drop(labels=['Category'], axis=1)\n",
    "predict_data['Category'] = predictions\n",
    "\n",
    "predict_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Dataframe\n",
    "predict_data.to_excel('CAC-2022-Predicted-Data-Set.xlsx')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ben_Gonzalez_CAC2022_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
